\section{Overview of the proposed framework}

In classical on-line planning, execution monitoring is used to determine the correct execution of the planned action. When some action fails the execution monitoring is able to determine a new initial state and request for a re-planning to the planner component from this new initial state.
In general, this mechanism does not change the model of the domain (i.e., the domain description), therefore this approach is suitable as soon as the cause of the failure is somewhat modeled in the domain description, in such a way that a new re-planning will avoid this situation.

However, very often, it is not possible to anticipate and model all the possible causes of failures for the actions. In these cases, the planner may not be able to find for alternative solutions with respect to the ones that failed. For example, in a domain in which a robot can open doors with its arms and in which the fact that doors may be locked is not modeled in the domain description, a plan can be generated containing an action to open a door. When this action fails, the planner would re-plan to open this door again, since the fact that it must be unlocked is not modeled in the preconditions of this action.
In this case, it would be needed to upgrade the model adding a representation of the property of a door to be locked or not.


The general approach that we will follow in this work includes three general modules: a planner, an executor, and a model updater. The three modules will cooperate during the execution of a complex task for a robot, including many failures of the planned actions, providing for a feedback mechanism from execution to planning.
More specifically, the following interactions are devised:
1) the planner notifies on-line to the executor the best plan (policy) to be executed according to the current model of the world; 2) the executor executes this plan (policy) and determines success or failures of the actions; 3) each failure is reported to the model updater that will follow some rules (either automatic domain dependent or manual domain dependent) to modify the current model, so that the planner can generate a new plan that is more suitable for the current situation as detected by the executor.

It is important to remark that the update of the model must be done in such a way to not increase the size and the complexity of the model at every cycle and for every failure situation encountered. Indeed, this would easily bring to a too large and not scalable model update process. Instead, our proposal is to separate the representations at the model level and at the execution level, in order to have a more abstract representation at the model level and a finer representation at execution level.
In other words, not all the causes of failure will be represented at the model level. This guarantees scalability of the overall system to many possible reasons of action failures,
since they will not increase the size and the complexity of the model.

In order to implement this approach we have chosen Markov Decision Processes (MDP) as planning formalism, Petri Net Plans (PNP) as execution formalism, and a rule based system as the model updater formalism.

\section{Preliminaries}

\subsection{MDP}

Markov Decision Processes model sequential decision problems in probabilistic environments. More Formally, we have $M=(S,A,T,R)$ where $S$ is a finite set of states, $A$ is a set of actions, $T(s,a,s')$ a transition function denoting the probability for going from state $s$ to state $s'$ using action $a$, and $R(s,a,s')$ a reward function expressing the expected gain for using action $a$ in state $s$ and arriving in state $s'$.

The Markov property tells that the future only depends on the current state. That implies that the state alone must be sufficient for deciding on what to do.
Moreover, states are \emph{observable}: at any time, the system knows what the current state is, even if transitions are probabilistic. For example a dice may run any value between 1 and 6 with probability $1/6$. However, when the dice has been thrown, we can read its actual value.

MDPs can be solved efficiently using standard algorithms like \emph{Value Iteration}. The resulting policy $\pi(s)=a$ gives the optimal action for any state $s$. It maximises the expected value $V(s)=\sum_{t=0}^H{\gamma^t \sum_{s'}{R(s_t,\pi(s_t),s') T(s_t,\pi(s_t),s')}}$ over the next $H$ (the horizon) actions, with $0<\gamma\le1$ a discount factor.

\subsection{PNP}

\subsection{Rule based system}

\section{Activity Descriptions}

The overall description of robot's goals and activities is formed by two parts:
1) Progressive Reasonning Units (PRU) \cite{CaMoZi01}, denoting the tasks to be executed;
2) Execution Rules (ER), denoting execution conditions, action failures and recovery procedures for these tasks.

The overall procedure of the working system is the following:
\begin{enumerate}
\item From a PRU, a first MDP $M$ is generated 
\item Given the model $M$, the best policy $\pi$ is computed through a Value-iteration algorithm
\item The best policy $\pi$ is transformed in a PNP and it is executed in a real setting
\item If some failure occurs, information about this failure are computed and sent to the model updater component
\item The model updater component generates an updated model $M'$, given the current model $M$, the outcome of the execution of the current policy $\pi$ and the Execution Rules
\item $M \leftarrow M'$ and goto step 2. 
\end{enumerate}



\subsection{Progressive Reasonning Units (PRU)}

Progressive Reasonning Units \cite{MZDecai98ws} are a family of models allowing for anytime computation, that is an aglorithm able to give a solution at any time, with an response quality increasing with time. In their classical form, they allow for solving a dynamic set of problems within a given deadline.
Each problem is described by a PRU, that is a sequence of computation levels. Each level gives a specific quality to the solution, but costs a certain amount of time. The main question of the algorithm is ``should I improve the solution of the current problem, or skip to next problem?''.

In advanced versions, a given PRU level may include several alternative modules with specific durations and qualities, and even resource consumption. Anyway, these are still dedicated to computing and enhancing some result before skipping to next one. In order to apply these techniques to more general problems, we had to refine and extend the PRU concept.

\subsection{Extended PRUs}

In this work we have extended the definition of PRUs in order to add the desired features. The new formalization (that we call PRU+) is this described in this section.

A PRU+ is a PRU where each level is given a set of state variables describing the current state of the problem to be solved. These variables may be different from one level to the other. The problem can be computational, but it can include any process where progressive stages can be acheived, like in a hierarchical plan. For example, to read the newspaper, you have to go to the mailbox, take the newspaper, get back and start reading. Each stage has different processing details that can be dealt with independantly.

More formally, a PRU+ is composed by a sequence of \emph{processing levels} $L = ( l_1, l_2, \ldots, l_{|L|})$, a set of \emph{state variables} $X = \{ X_1, \ldots, X_{|X|} \}$, and a set of \emph{observable boolean properties} of the environment $O = \{ o_1, \ldots, o_{|O|} \}$.
Each state variable $X_i$ can be assigned to a value within a set of finite values, i.e., $X_i \in A_i = \{ \bot , a_i^1, \ldots, a_i^{|A_i|} \}$, with $|A_i|$ finite, and $\bot$ denoting a special null value. Each observable property can be evaluated at execution time by an external function that is always able to return a truth value for it.

Each processing level $l_i$ is composed by a set of \emph{modules} $( m_i^1, \ldots, m_i^{p_i} )$ and it is associated to a set of active state variables $V_i \subseteq X$.
We denote with $M$ the set of all the modules in all levels.

Each module $m_i^j$ is defined by a non-empty set of \emph{options} $\{ \alpha_i^j, \beta_i^j, \ldots \}$, representing possible outcomes of its execution. The symbols used to denote an option (e.g., $\alpha_i^j$) are assumed to be unique identifiers in all the PRU+.

Each option $\alpha_i^j$ contains the following information:
\begin{itemize}
\item \emph{execution condition} $\alpha_i^j.\phi$: a logical formula over atoms in $O$ and equality checks of values in state variables; this formula denotes an observable condition used at execution time to recognize this outcome; we assume that all the conditions for the options of a given module are mutually exclusive;
\item \emph{probability} $\alpha_i^j.p$: probability of occurrence of this outcome; the sum of all the probability values for all the options in a module is 1;
\item \emph{quality} $\alpha_i^j.q$: estimated quality for achieving this outcome ($q$ can be expressed either as a constant value or as a function pointer (see below);
\item \emph{duration} $\alpha_i^j.d$: estimation time duration for achieving this outcome ($d$ can be expressed either as a constant value or as a function pointer (see below);
\item \emph{successor modules} $\alpha_i^j.SM$: a set of successor modules that are enables after this outcome; for each $m_k^* \in \alpha_i^j.SM$, we have $k \geq j$, so successor modules should be either at the same level of at a next level with respect to the current one;
\item \emph{state variable updates} $\alpha_i^j.SVU$: a set of state variable assignments that must be considered after this outcome; for each $(X_k \leftarrow a'_k) \in \alpha_i^j.SVU$, $X_i \in V_i$ (only state variables active for the current level are allowed) and  $a'_i \in A_i$.
\end{itemize}

Given a PRU+, it is possible to transform it in a MDP in order to compute an optimal policy.

The representation of the states $S$ for this MDP is defined as follows.
Each state in $S$ is defined by $[  \alpha_i^j, X_1, \ldots, X_{|X|} ]$, where
$X_k = \bot$ for all the nonactive state variables for this level, i.e.,$ X_k \notin V_i$.
An initial state $S_0 = [ \alpha_0^0, \bot, \ldots, \bot ]$ is also defined in the set of states $S$, with $\alpha_0^0$ being a special symbol used only in the representation of the initial state. This state describes the modules that can be used at the beginning. Finally, states that are achieved after the execution of modules in the last level are marked as goal states and grouped in a set of goal states $G \subset S$.
Notice that in the proposed framework, the states do not have to be fully observable at any time, since at execution time only the observable properties and the state variables used in the execution conditions are required to determine the state of the execution of the policy. However each transition must be fully observable since the execution module needs to know which option has been activated at runtime.

Actions in MDP corresponds to the modules in the PRU+. 

The transition function $\delta$ of the MDP is defined according to the probabilities and the set of successor modules defined in the PRU.
Thus we can define the transition function for the initial state $S_0$ as follows: 
$Pr(S_0,m_1^j,S(\alpha_1^j)) = \alpha_1^j.p$, where
$S(\alpha_1^j)$ is the state resulting from the corresponding outcome $\alpha_1^j$ of $m_1^j$.
More specifically, $S(\alpha_1^j) = [ \alpha_1^j, e_1^{j,(1)}, \ldots, e_1^{j,(|X|)} ]$,
with
 
\[ e_1^{j,(k)} = \left\{ 
\begin{tabular}{l} 
$\bot$ if $X_k \notin V_1$, (i.e., non active state variables) \\  
$a'_k$ if $(X_k \leftarrow a'_k) \in \alpha_1^j.SVU$, (i.e., updated by this action) \\
$a_k$  if all the predecessors have the value $a_k$ \\
$\bot$ if inconsistency in the values of the predecessors \\
\end{tabular} \right. \]

This process can be repeated at any level to fully define the transition function.


Estimated quality and estimated time duration of an option of a module can be represented either as a constant value or as a function pointer. In the latter case, we define the following standard function prototype: $f : S \times M \times S \times P \times \Re \rightarrow \Re$ for both the quality and the duration function. More precisely, the 5 arguments of the function prototype have the following meaning: 1) current state, 2) current action, 3) successor state, 4) function type, 5) additional. Notice that these functions are called at planning time to estimate the quality and the duration of an action executed from a virtual current state to a virtual successor state. These functions (being obviously domain dependent) are implemented by the developer of the action that should return an estimate of quality and duration in the  conditions set as arguments. The function type and the additional values are parameters used to define general functions that can be used in a parametric way by different modules.
The returned values must be expressed in the same measuring units by all the modules. In particular, the duration is the estimated time to complete the action in seconds.

\subsection{Computing the optimal policy}

Computing the optimal policy simply consists in solving the MDP model. Classical algorithms like \emph{Value Iteration} \cite{Bellman57} or \emph{Policy Iteration} \cite{howard1960dynamic} are able to compute an optimal policy efficiently.

\todo{any need for more detail here?}


\subsection{Policy to PNP transformation}

The policy found by the MDP planner is transformed into a PNP by an algorithm that applies the PNP operators. 

The output of the MDP planner contains the initial and goal states,  the optimal policy, and the possible successor states for each execution of an action in the policy. In other words, this output corresponds to the optimal policy and to a portion of the MDP model: initial and goal states and a subset of the transition function of the MDP containing information only for the pairs (state, action) that are considered in the returned policy.

More precisely, the output of the MDP planner is $\langle S_0, G, \{ \langle s_i, a_i, SS_i \rangle \} \rangle $,  
where $S_0$ is the initial state, $G$ is a set of goal states (i.e., final states),
and in each tuple $\langle s_i, a_i, SS_i \rangle$, $s_i$ is a state, $a_i$ is the action to be executed in this state, $SS_i$ is a set of pairs $(s_i^k, \phi_i^k)$, with $s_i^k$ being a successor state and $\phi_i^k$ a formula over observable properties of the environment (including state variables) that  must be evaluated to determine the actual outcome of the execution of $a_i$ in $s_i$.
In this formalization, it is thus not necessary to fully observe the states $s_i^k$, but only to determine the value of the conditions $\phi_i^k$.

The PNP is built by navigating the policy from the initial state to the goal states.
A standard approach to build a graph using a queue of states and a vector of visited states as data structures is used.
The queue is initialized with the initial state $S_0$ which is also added as the initial place in the PNP.
While the queue is not empty, the PNP sequence operator is used to add the action $a_i$ that must be executed in the current state $s_i$ (according to the policy), followed by a set of transitions that will check the conditions $\phi_i^k$ bringing to any of the states $s_i^k$, as defined in $SS_i$. 
If a state in $SS_i$ has not been visited, then a new place is generated in the PNP, this place is associated with this state that is now marked as visited, and this state is added to the queue. If a state in $SS_i$ has been already visited, then the transition is connected with the place that was stored when the state was created. The process thus continues until the queue is empty.
When encountering goal states in $G$ the corresponding places are labeled as ``goals'' that will be used by the PNP executor to terminate the execution of the plan and return a ``success'' feedback.

\todo{describe the algorithm for generating the PNP in details.}



It is important to observe that the descriptions of the successor states in the output returned by the MDP planner are generated according to the specification of the PRU, as described in the previous section. Thus, they contain the conditions $\phi_i^k$ over observable properties of the environment that can be actually checked at run time to determine the current outcome of actions and thus the correct successor state from which the execution should continue.

% AT THIS MOMENT LET'S CONSIDER ONLY MUTUALLY-EXCLUSIVE OPTIONS
%Obviously, the descriptions of the states generated in this way are not complete, but they contain only those predicates that are needed by the next action. {\bf ONLY THE NEXT ONE OR ALSO THE OTHERS???}
%Therefore the set of successor states in the output policy may contain states described with conditions that are not mutually exclusive. In particular, it may include state descriptions that implies other state descriptions. For example, a possible set of successor states may be 
%$\{ A, A \wedge B \}$. If these conditions are added to the PNP executor without any execution rule, then when $A$ and $B$ are both true, both the transitions will be enabled and the behavior of the PNP will be non-deterministic. However, in this situation, the MDP model requires that, if both $A$ and $B$ are true, then the effect $A \wedge B$ must be considered, because it is the most specific.
%This behavior is thus also implemented in the PNP executor. Whenever multiple transitions are enabled, the most specific one is selected and will fire.

%Notice that when multiple non-exclusive conditions are present in the description of successor states and they are not in relation given by logical implication, then a non-deterministic execution of the transition in the PNP executor will happen, but in this case it will be consistent with the MDP model.



\subsection{Execution Rules (ER)}

Execution rules contains several different types of rules that are listed below.

\vspace{0.5cm}

1. Execution conditions for actions:

\[ \mathbf{do} \;\; \emph{action} \;\; \mathbf{while} \;\; ( \emph{condition} ) \]

\noindent
the action can be executed as long as \emph{condition} is verified.
If \emph{condition} is false, the action might fail or its performance or probability of success might degrade.
A special condition, \emph{user\_interrupt}, is used to model the possibility for a user to interrupt the action.

\vspace{0.5cm}

2. Recovery procedures

\[ \mathbf{if} \;\; \emph{action} \;\; \mathbf{fails, try} \;\; \emph{recovery} \;\; \]

\noindent
if the action fails a recovery procedure (i.e., a pre-defined PNP) is tried, typically in order to bring the system back to a known safe state.


\vspace{0.5cm}

3. Update procedures

\[ \mathbf{if} \;\; \emph{action} \;\; \mathbf{fails, then} \;\; \emph{drop\_action} \;\; \]
\[ \mathbf{if} \;\; \emph{action} \;\; \mathbf{fails, then} \;\; \emph{reduce\_transition\_probability} \;\; \]
\[ \mathbf{if} \;\; \emph{action} \;\; \mathbf{fails, then} \;\; \emph{reduce\_reward} \;\; \]



\noindent
if the action fails, the MDP will be modified according to one or more of these rules.






\subsection{Model Update}


\todo{describe the method for updating the MDP model based on the execution results.}


%\section{Examples: Assistance and Escorting Tasks}



